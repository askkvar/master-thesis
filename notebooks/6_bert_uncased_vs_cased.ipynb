{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification Model 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Trained on IMDB dataset, predicts on WW2 Bunker Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from transformers import (\n",
    "    BertTokenizer, \n",
    "    BertForSequenceClassification, \n",
    "    Trainer, \n",
    "    TrainingArguments\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_recall_fscore_support, f1_score\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>{'input_ids': [101, 2028, 1997, 1996, 2060, 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "      <td>A wonderful little production. The filming tec...</td>\n",
       "      <td>{'input_ids': [101, 1037, 6919, 2210, 2537, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>{'input_ids': [101, 1045, 2245, 2023, 2001, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>{'input_ids': [101, 10468, 2045, 1005, 1055, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "      <td>Petter Mattei's Love in the Time of Money is a...</td>\n",
       "      <td>{'input_ids': [101, 9004, 3334, 4717, 7416, 10...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment  \\\n",
       "0  One of the other reviewers has mentioned that ...          1   \n",
       "1  A wonderful little production. <br /><br />The...          1   \n",
       "2  I thought this was a wonderful way to spend ti...          1   \n",
       "3  Basically there's a family where a little boy ...          0   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...          1   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  One of the other reviewers has mentioned that ...   \n",
       "1  A wonderful little production. The filming tec...   \n",
       "2  I thought this was a wonderful way to spend ti...   \n",
       "3  Basically there's a family where a little boy ...   \n",
       "4  Petter Mattei's Love in the Time of Money is a...   \n",
       "\n",
       "                                              tokens  \n",
       "0  {'input_ids': [101, 2028, 1997, 1996, 2060, 15...  \n",
       "1  {'input_ids': [101, 1037, 6919, 2210, 2537, 10...  \n",
       "2  {'input_ids': [101, 1045, 2245, 2023, 2001, 10...  \n",
       "3  {'input_ids': [101, 10468, 2045, 1005, 1055, 1...  \n",
       "4  {'input_ids': [101, 9004, 3334, 4717, 7416, 10...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(\"../data/processed/processed_movie_reviews.pkl\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare Train/Validation Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 30000\n",
      "Validation samples: 10000\n",
      "Test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "    df[\"review\"], df[\"sentiment\"], test_size=0.4, random_state=42, stratify=df[\"sentiment\"]\n",
    ")\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "    temp_texts, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n",
    ")\n",
    "\n",
    "print(\"Training samples:\", len(train_texts))\n",
    "print(\"Validation samples:\", len(val_texts))\n",
    "print(\"Test samples:\", len(test_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create a Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        # texts and labels can be lists or pd.Series\n",
    "        self.texts = texts.tolist() if hasattr(texts, \"tolist\") else texts\n",
    "        self.labels = labels.tolist() if hasattr(labels, \"tolist\") else labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Instansiate Tokenizer & Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer for the uncased model\n",
    "tokenizer_uncased = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# Tokenizer for the cased model\n",
    "tokenizer_cased = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Create datasets for each tokenizer.\n",
    "train_dataset_uncased = SentimentDataset(train_texts, train_labels, tokenizer_uncased)\n",
    "val_dataset_uncased = SentimentDataset(val_texts, val_labels, tokenizer_uncased)\n",
    "test_dataset_uncased = SentimentDataset(test_texts, test_labels, tokenizer_uncased)\n",
    "\n",
    "train_dataset_cased = SentimentDataset(train_texts, train_labels, tokenizer_cased)\n",
    "val_dataset_cased = SentimentDataset(val_texts, val_labels, tokenizer_cased)\n",
    "test_dataset_cased = SentimentDataset(test_texts, test_labels, tokenizer_cased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Initialize Model and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/askk/.local/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/cluster/home/askk/.local/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args_uncased = TrainingArguments(\n",
    "    output_dir=\"outputs_uncased\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"logs_uncased\",\n",
    "    logging_steps=50,\n",
    "    fp16=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "training_args_cased = TrainingArguments(\n",
    "    output_dir=\"outputs_cased\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"logs_cased\",\n",
    "    logging_steps=50,\n",
    "    fp16=True if torch.cuda.is_available() else False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_uncased = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "model_cased = BertForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_uncased = Trainer(\n",
    "    model=model_uncased,\n",
    "    args=training_args_uncased,\n",
    "    train_dataset=train_dataset_uncased,\n",
    "    eval_dataset=val_dataset_uncased\n",
    ")\n",
    "\n",
    "trainer_cased = Trainer(\n",
    "    model=model_cased,\n",
    "    args=training_args_cased,\n",
    "    train_dataset=train_dataset_cased,\n",
    "    eval_dataset=val_dataset_cased\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training bert-base-uncased model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11250' max='11250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11250/11250 26:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.296400</td>\n",
       "      <td>0.233284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.137600</td>\n",
       "      <td>0.320224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.060300</td>\n",
       "      <td>0.323376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=11250, training_loss=0.19302278794050218, metrics={'train_runtime': 1592.2376, 'train_samples_per_second': 56.524, 'train_steps_per_second': 7.066, 'total_flos': 2.36799949824e+16, 'train_loss': 0.19302278794050218, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Training bert-base-uncased model...\")\n",
    "trainer_uncased.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training bert-base-cased model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11250' max='11250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11250/11250 24:47, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.313300</td>\n",
       "      <td>0.270315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.183500</td>\n",
       "      <td>0.319120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.112300</td>\n",
       "      <td>0.337604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=11250, training_loss=0.23029671374956767, metrics={'train_runtime': 1487.2043, 'train_samples_per_second': 60.516, 'train_steps_per_second': 7.565, 'total_flos': 2.36799949824e+16, 'train_loss': 0.23029671374956767, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Training bert-base-cased model...\")\n",
    "trainer_cased.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Uncased predictions\n",
    "predictions_uncased = trainer_uncased.predict(test_dataset_uncased)\n",
    "pred_labels_uncased = predictions_uncased.predictions.argmax(axis=1)\n",
    "\n",
    "# Cased predictions\n",
    "predictions_cased = trainer_cased.predict(test_dataset_cased)\n",
    "pred_labels_cased = predictions_cased.predictions.argmax(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model Comparison ===\n",
      "bert-base-uncased -> Accuracy: 0.9367, F1 (macro): 0.9367\n",
      "bert-base-cased   -> Accuracy: 0.9299, F1 (macro): 0.9299\n",
      "\n",
      "Classification Report (Uncased):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94      5000\n",
      "           1       0.94      0.94      0.94      5000\n",
      "\n",
      "    accuracy                           0.94     10000\n",
      "   macro avg       0.94      0.94      0.94     10000\n",
      "weighted avg       0.94      0.94      0.94     10000\n",
      "\n",
      "Classification Report (Cased):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93      5000\n",
      "           1       0.93      0.93      0.93      5000\n",
      "\n",
      "    accuracy                           0.93     10000\n",
      "   macro avg       0.93      0.93      0.93     10000\n",
      "weighted avg       0.93      0.93      0.93     10000\n",
      "\n",
      "Number of test samples where the models disagree: 498\n",
      "Examples of disagreements:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>pred_uncased</th>\n",
       "      <th>pred_cased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21065</th>\n",
       "      <td>STAR RATING: ***** Saturday Night **** Friday ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31445</th>\n",
       "      <td>Ok, so, this is coming a few weeks late, but i...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24070</th>\n",
       "      <td>After watching this film I experienced a new s...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46495</th>\n",
       "      <td>I got a good laugh reading all the idiotic com...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7145</th>\n",
       "      <td>i've just read the most recent remarks about t...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20726</th>\n",
       "      <td>It is more a subtle story of the fact that in ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2902</th>\n",
       "      <td>So.. what can I tell you about this movie. If ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5189</th>\n",
       "      <td>The new voices scare me! Kuzco doesn't have to...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31834</th>\n",
       "      <td>This is a movie that demonstrates that mood an...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11521</th>\n",
       "      <td>Steven Segal has done some awful films, but th...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  label  pred_uncased  \\\n",
       "21065  STAR RATING: ***** Saturday Night **** Friday ...      0             1   \n",
       "31445  Ok, so, this is coming a few weeks late, but i...      1             0   \n",
       "24070  After watching this film I experienced a new s...      1             1   \n",
       "46495  I got a good laugh reading all the idiotic com...      1             0   \n",
       "7145   i've just read the most recent remarks about t...      1             1   \n",
       "20726  It is more a subtle story of the fact that in ...      1             0   \n",
       "2902   So.. what can I tell you about this movie. If ...      0             0   \n",
       "5189   The new voices scare me! Kuzco doesn't have to...      1             0   \n",
       "31834  This is a movie that demonstrates that mood an...      0             1   \n",
       "11521  Steven Segal has done some awful films, but th...      0             0   \n",
       "\n",
       "       pred_cased  \n",
       "21065           0  \n",
       "31445           1  \n",
       "24070           0  \n",
       "46495           1  \n",
       "7145            0  \n",
       "20726           1  \n",
       "2902            1  \n",
       "5189            1  \n",
       "31834           0  \n",
       "11521           1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a DataFrame for the test set\n",
    "df_test = pd.DataFrame({\n",
    "    \"review\": test_texts,   # The text of the review\n",
    "    \"label\": test_labels    # The true label\n",
    "})\n",
    "\n",
    "# Add columns for predictions from both models\n",
    "df_test[\"pred_uncased\"] = pred_labels_uncased\n",
    "df_test[\"pred_cased\"] = pred_labels_cased\n",
    "\n",
    "# Calculate metrics for each model\n",
    "acc_uncased = accuracy_score(df_test[\"label\"], df_test[\"pred_uncased\"])\n",
    "f1_uncased = f1_score(df_test[\"label\"], df_test[\"pred_uncased\"], average=\"macro\")\n",
    "\n",
    "acc_cased = accuracy_score(df_test[\"label\"], df_test[\"pred_cased\"])\n",
    "f1_cased = f1_score(df_test[\"label\"], df_test[\"pred_cased\"], average=\"macro\")\n",
    "\n",
    "print(\"=== Model Comparison ===\")\n",
    "print(f\"bert-base-uncased -> Accuracy: {acc_uncased:.4f}, F1 (macro): {f1_uncased:.4f}\")\n",
    "print(f\"bert-base-cased   -> Accuracy: {acc_cased:.4f}, F1 (macro): {f1_cased:.4f}\\n\")\n",
    "\n",
    "# Optional: Print a detailed classification report for each\n",
    "print(\"Classification Report (Uncased):\")\n",
    "print(classification_report(df_test[\"label\"], df_test[\"pred_uncased\"]))\n",
    "print(\"Classification Report (Cased):\")\n",
    "print(classification_report(df_test[\"label\"], df_test[\"pred_cased\"]))\n",
    "\n",
    "# Identify rows where the two models disagree\n",
    "df_diff = df_test[df_test[\"pred_uncased\"] != df_test[\"pred_cased\"]]\n",
    "print(f\"Number of test samples where the models disagree: {len(df_diff)}\")\n",
    "\n",
    "# Display a few examples of disagreements\n",
    "print(\"Examples of disagreements:\")\n",
    "display(df_diff[[\"review\", \"label\", \"pred_uncased\", \"pred_cased\"]].head(10))\n",
    "\n",
    "# Visualize the disagreement distribution using a count plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(data=df_diff, x=\"pred_uncased\", hue=\"pred_cased\")\n",
    "plt.title(\"Count of Disagreements between Uncased and Cased Predictions\")\n",
    "plt.xlabel(\"bert-base-uncased Prediction\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(title=\"bert-base-cased Prediction\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert-base-uncased prediction on ww2 sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training set: 773 reviews\n",
      "Filtered training set (only positive and negative): 654 reviews\n",
      "Training samples: 523\n",
      "Validation samples: 131\n",
      "Class weights: [7.26388889 0.53696099]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/askk/.local/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning bert-base-uncased on binary bunker reviews with class weighting...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='198' max='198' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [198/198 00:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.574400</td>\n",
       "      <td>0.752057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.301000</td>\n",
       "      <td>0.637851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.075800</td>\n",
       "      <td>0.674478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# 1. Data Preparation\n",
    "# -----------------------------\n",
    "df_train = pd.read_pickle(\"../data/processed/bunker_reviews_fine_tuning.pkl\")\n",
    "print(\"Original training set:\", len(df_train), \"reviews\")\n",
    "\n",
    "# Filter out neutral reviews; keep only \"positive\" and \"negative\"\n",
    "df_train = df_train[df_train[\"manual_classification\"].isin([\"positive\", \"negative\"])].copy()\n",
    "print(\"Filtered training set (only positive and negative):\", len(df_train), \"reviews\")\n",
    "\n",
    "# Map labels: negative â†’ 0, positive â†’ 1\n",
    "label_mapping = {\"negative\": 0, \"positive\": 1}\n",
    "df_train[\"binary_label\"] = df_train[\"manual_classification\"].map(label_mapping)\n",
    "\n",
    "# Extract texts and labels\n",
    "train_texts = df_train[\"clean_text\"].tolist()\n",
    "train_labels = df_train[\"binary_label\"].tolist()\n",
    "\n",
    "# Split into training and validation sets (60/20/20 split)\n",
    "train_texts_split, val_texts_split, train_labels_split, val_labels_split = train_test_split(\n",
    "    train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels\n",
    ")\n",
    "print(\"Training samples:\", len(train_texts_split))\n",
    "print(\"Validation samples:\", len(val_texts_split))\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Compute Class Weights\n",
    "# -----------------------------\n",
    "classes = np.unique(train_labels_split)\n",
    "class_weights = compute_class_weight(\"balanced\", classes=classes, y=train_labels_split)\n",
    "print(\"Class weights:\", class_weights)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Define a Custom Dataset Class\n",
    "# -----------------------------\n",
    "class SentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts if isinstance(texts, list) else texts.tolist()\n",
    "        self.labels = labels if isinstance(labels, list) else labels.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Define a Custom Trainer with Weighted Loss\n",
    "# -----------------------------\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        # Remove labels from inputs and get them separately\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)  # forward pass\n",
    "        logits = outputs.logits     # model predictions\n",
    "\n",
    "        # Create a weighted CrossEntropyLoss using the computed class weights\n",
    "        loss_fct = CrossEntropyLoss(weight=class_weights_tensor.to(model.device))\n",
    "        loss = loss_fct(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Create Tokenizer and Datasets\n",
    "# -----------------------------\n",
    "tokenizer_uncased = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "train_dataset = SentimentDataset(train_texts_split, train_labels_split, tokenizer_uncased)\n",
    "val_dataset = SentimentDataset(val_texts_split, val_labels_split, tokenizer_uncased)\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Set Up Training Arguments\n",
    "# -----------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"outputs_bunker_binary_uncased\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",  # Note: will be deprecated in future versions; use eval_strategy later.\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    learning_rate=2e-5,\n",
    "    fp16=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Load Model\n",
    "# -----------------------------\n",
    "model_uncased = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Initialize Custom Trainer and Fine-Tune\n",
    "# -----------------------------\n",
    "trainer_uncased = WeightedTrainer(\n",
    "    model=model_uncased,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "print(\"Fine-tuning bert-base-uncased on binary bunker reviews with class weighting...\")\n",
    "trainer_uncased.train()\n",
    "trainer_uncased.save_model(\"outputs_bunker_binary_uncased/final_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original test set: 194 reviews\n",
      "Filtered test set (only positive and negative): 172 reviews\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to ../data/processed/bunker_predictions_binary.csv\n",
      "Accuracy: 0.9767\n",
      "Confusion Matrix:\n",
      "[[154   2]\n",
      " [  2  14]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.99      0.99      0.99       156\n",
      "    negative       0.88      0.88      0.88        16\n",
      "\n",
      "    accuracy                           0.98       172\n",
      "   macro avg       0.93      0.93      0.93       172\n",
      "weighted avg       0.98      0.98      0.98       172\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_uncased = BertForSequenceClassification.from_pretrained(\"outputs_bunker_binary_uncased/final_model\", num_labels=2)\n",
    "tokenizer_uncased = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Load the test dataset (this may contain all reviews including neutral)\n",
    "df_test = pd.read_pickle(\"../data/processed/bunker_reviews_test_set.pkl\")\n",
    "print(\"Original test set:\", len(df_test), \"reviews\")\n",
    "\n",
    "# Filter the test set to only include reviews with manual classifications \"positive\" or \"negative\"\n",
    "df_test_filtered = df_test[df_test[\"manual_classification\"].isin([\"positive\", \"negative\"])].copy()\n",
    "print(\"Filtered test set (only positive and negative):\", len(df_test_filtered), \"reviews\")\n",
    "\n",
    "# Map labels in the test set for evaluation (if manual_classification exists)\n",
    "df_test_filtered[\"binary_label\"] = df_test_filtered[\"manual_classification\"].map(label_mapping)\n",
    "\n",
    "# Prepare the texts and true labels\n",
    "test_texts = df_test_filtered[\"clean_text\"].tolist()\n",
    "true_labels = df_test_filtered[\"binary_label\"].tolist()\n",
    "\n",
    "# Define an inference dataset\n",
    "class InferenceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=512):\n",
    "        self.texts = texts if isinstance(texts, list) else texts.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze()\n",
    "        }\n",
    "\n",
    "inference_dataset = InferenceDataset(test_texts, tokenizer_uncased)\n",
    "\n",
    "# Set up evaluation TrainingArguments\n",
    "eval_training_args = TrainingArguments(\n",
    "    output_dir=\"outputs_bunker_binary_uncased\",\n",
    "    per_device_eval_batch_size=8\n",
    ")\n",
    "\n",
    "# Initialize Trainer (we only need the model and the arguments for prediction)\n",
    "trainer = Trainer(\n",
    "    model=model_uncased,\n",
    "    args=eval_training_args\n",
    ")\n",
    "\n",
    "# Get predictions\n",
    "predictions_output = trainer.predict(inference_dataset)\n",
    "logits = predictions_output.predictions  # shape: (num_samples, 2)\n",
    "probs = F.softmax(torch.tensor(logits), dim=1).numpy()\n",
    "\n",
    "# Map binary predictions to string labels (0: negative, 1: positive)\n",
    "label_mapping_str = {0: \"negative\", 1: \"positive\"}\n",
    "final_preds = [label_mapping_str[p.argmax()] for p in probs]\n",
    "\n",
    "# Add predictions to the test DataFrame\n",
    "df_test_filtered[\"predicted_sentiment\"] = final_preds\n",
    "\n",
    "# Optionally reorder columns (adjust if necessary)\n",
    "df_test_filtered = df_test_filtered[[\"clean_text\", \"manual_classification\", \"predicted_sentiment\", \"tokens\"]]\n",
    "\n",
    "# Save predictions to CSV\n",
    "df_test_filtered.to_csv(\"../data/processed/bunker_predictions_binary.csv\", index=False)\n",
    "print(\"Predictions saved to ../data/processed/bunker_predictions_binary.csv\")\n",
    "\n",
    "# Evaluate performance (if gold labels are available)\n",
    "true_labels_str = [label_mapping_str[label] for label in true_labels]\n",
    "accuracy = accuracy_score(true_labels_str, final_preds)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "cm = confusion_matrix(df_test_filtered[\"manual_classification\"], df_test_filtered[\"predicted_sentiment\"], labels=[\"positive\", \"negative\"])\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "report = classification_report(df_test_filtered[\"manual_classification\"], df_test_filtered[\"predicted_sentiment\"], labels=[\"positive\", \"negative\"])\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert-base-cased prediction on ww2 sample dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training set: 773 reviews\n",
      "Filtered training set (only positive and negative): 654 reviews\n",
      "Training samples: 523\n",
      "Validation samples: 131\n",
      "Class weights: [7.26388889 0.53696099]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/askk/.local/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning bert-base-cased on binary bunker reviews with class weighting...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='198' max='198' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [198/198 00:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.608100</td>\n",
       "      <td>0.586244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.463700</td>\n",
       "      <td>0.729826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.093900</td>\n",
       "      <td>0.607110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# 1. Data Preparation\n",
    "# -----------------------------\n",
    "df_train = pd.read_pickle(\"../data/processed/bunker_reviews_fine_tuning.pkl\")\n",
    "print(\"Original training set:\", len(df_train), \"reviews\")\n",
    "\n",
    "# Filter out neutral reviews; keep only \"positive\" and \"negative\"\n",
    "df_train = df_train[df_train[\"manual_classification\"].isin([\"positive\", \"negative\"])].copy()\n",
    "print(\"Filtered training set (only positive and negative):\", len(df_train), \"reviews\")\n",
    "\n",
    "# Map labels: negative â†’ 0, positive â†’ 1\n",
    "label_mapping = {\"negative\": 0, \"positive\": 1}\n",
    "df_train[\"binary_label\"] = df_train[\"manual_classification\"].map(label_mapping)\n",
    "\n",
    "# Extract texts and labels\n",
    "train_texts = df_train[\"clean_text\"].tolist()\n",
    "train_labels = df_train[\"binary_label\"].tolist()\n",
    "\n",
    "# Split into training and validation sets (60/20/20 split)\n",
    "train_texts_split, val_texts_split, train_labels_split, val_labels_split = train_test_split(\n",
    "    train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels\n",
    ")\n",
    "print(\"Training samples:\", len(train_texts_split))\n",
    "print(\"Validation samples:\", len(val_texts_split))\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Compute Class Weights\n",
    "# -----------------------------\n",
    "classes = np.unique(train_labels_split)\n",
    "class_weights = compute_class_weight(\"balanced\", classes=classes, y=train_labels_split)\n",
    "print(\"Class weights:\", class_weights)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Define a Custom Dataset Class\n",
    "# -----------------------------\n",
    "class SentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts if isinstance(texts, list) else texts.tolist()\n",
    "        self.labels = labels if isinstance(labels, list) else labels.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Define a Custom Trainer with Weighted Loss\n",
    "# -----------------------------\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        # Remove labels from inputs and get them separately\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)  # forward pass\n",
    "        logits = outputs.logits     # model predictions\n",
    "\n",
    "        # Create a weighted CrossEntropyLoss using the computed class weights\n",
    "        loss_fct = CrossEntropyLoss(weight=class_weights_tensor.to(model.device))\n",
    "        loss = loss_fct(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Create Tokenizer and Datasets\n",
    "# -----------------------------\n",
    "tokenizer_uncased = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "train_dataset = SentimentDataset(train_texts_split, train_labels_split, tokenizer_uncased)\n",
    "val_dataset = SentimentDataset(val_texts_split, val_labels_split, tokenizer_uncased)\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Set Up Training Arguments\n",
    "# -----------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"outputs_bunker_binary_cased\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",  # Note: will be deprecated in future versions; use eval_strategy later.\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    learning_rate=2e-5,\n",
    "    fp16=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Load Model\n",
    "# -----------------------------\n",
    "model_uncased = BertForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Initialize Custom Trainer and Fine-Tune\n",
    "# -----------------------------\n",
    "trainer_uncased = WeightedTrainer(\n",
    "    model=model_uncased,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "print(\"Fine-tuning bert-base-cased on binary bunker reviews with class weighting...\")\n",
    "trainer_uncased.train()\n",
    "trainer_uncased.save_model(\"outputs_bunker_binary_cased/final_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original test set: 194 reviews\n",
      "Filtered test set (only positive and negative): 172 reviews\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to ../data/processed/bunker_predictions_binary.csv\n",
      "Accuracy: 0.9767\n",
      "Confusion Matrix:\n",
      "[[155   1]\n",
      " [  3  13]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.98      0.99      0.99       156\n",
      "    negative       0.93      0.81      0.87        16\n",
      "\n",
      "    accuracy                           0.98       172\n",
      "   macro avg       0.95      0.90      0.93       172\n",
      "weighted avg       0.98      0.98      0.98       172\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_uncased = BertForSequenceClassification.from_pretrained(\"outputs_bunker_binary_cased/final_model\", num_labels=2)\n",
    "tokenizer_uncased = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Load the test dataset (this may contain all reviews including neutral)\n",
    "df_test = pd.read_pickle(\"../data/processed/bunker_reviews_test_set.pkl\")\n",
    "print(\"Original test set:\", len(df_test), \"reviews\")\n",
    "\n",
    "# Filter the test set to only include reviews with manual classifications \"positive\" or \"negative\"\n",
    "df_test_filtered = df_test[df_test[\"manual_classification\"].isin([\"positive\", \"negative\"])].copy()\n",
    "print(\"Filtered test set (only positive and negative):\", len(df_test_filtered), \"reviews\")\n",
    "\n",
    "# Map labels in the test set for evaluation (if manual_classification exists)\n",
    "df_test_filtered[\"binary_label\"] = df_test_filtered[\"manual_classification\"].map(label_mapping)\n",
    "\n",
    "# Prepare the texts and true labels\n",
    "test_texts = df_test_filtered[\"clean_text\"].tolist()\n",
    "true_labels = df_test_filtered[\"binary_label\"].tolist()\n",
    "\n",
    "# Define an inference dataset\n",
    "class InferenceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=512):\n",
    "        self.texts = texts if isinstance(texts, list) else texts.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze()\n",
    "        }\n",
    "\n",
    "inference_dataset = InferenceDataset(test_texts, tokenizer_uncased)\n",
    "\n",
    "# Set up evaluation TrainingArguments\n",
    "eval_training_args = TrainingArguments(\n",
    "    output_dir=\"outputs_bunker_binary_cased\",\n",
    "    per_device_eval_batch_size=8\n",
    ")\n",
    "\n",
    "# Initialize Trainer (we only need the model and the arguments for prediction)\n",
    "trainer = Trainer(\n",
    "    model=model_uncased,\n",
    "    args=eval_training_args\n",
    ")\n",
    "\n",
    "# Get predictions\n",
    "predictions_output = trainer.predict(inference_dataset)\n",
    "logits = predictions_output.predictions  # shape: (num_samples, 2)\n",
    "probs = F.softmax(torch.tensor(logits), dim=1).numpy()\n",
    "\n",
    "# Map binary predictions to string labels (0: negative, 1: positive)\n",
    "label_mapping_str = {0: \"negative\", 1: \"positive\"}\n",
    "final_preds = [label_mapping_str[p.argmax()] for p in probs]\n",
    "\n",
    "# Add predictions to the test DataFrame\n",
    "df_test_filtered[\"predicted_sentiment\"] = final_preds\n",
    "\n",
    "# Optionally reorder columns (adjust if necessary)\n",
    "df_test_filtered = df_test_filtered[[\"clean_text\", \"manual_classification\", \"predicted_sentiment\", \"tokens\"]]\n",
    "\n",
    "# Save predictions to CSV\n",
    "df_test_filtered.to_csv(\"../data/processed/bunker_predictions_binary.csv\", index=False)\n",
    "print(\"Predictions saved to ../data/processed/bunker_predictions_binary.csv\")\n",
    "\n",
    "# Evaluate performance (if gold labels are available)\n",
    "true_labels_str = [label_mapping_str[label] for label in true_labels]\n",
    "accuracy = accuracy_score(true_labels_str, final_preds)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "cm = confusion_matrix(df_test_filtered[\"manual_classification\"], df_test_filtered[\"predicted_sentiment\"], labels=[\"positive\", \"negative\"])\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "report = classification_report(df_test_filtered[\"manual_classification\"], df_test_filtered[\"predicted_sentiment\"], labels=[\"positive\", \"negative\"])\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
